# -*- coding: utf-8 -*-
"""EPQ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17UeCfNL9nJ_g7HGLQoghp4Jcv5g54mtB
"""

import tensorflow as tf
import os
from tensorflow.keras.layers import Conv2D, Input, BatchNormalization, Add, ReLU, Reshape, UpSampling2D, Rescaling
from tensorflow.keras.models import Model
import numpy as np
from tensorflow.keras import initializers
import PIL.Image
import functools
import pathlib
import scipy.misc as misc

dim = 256

def tensor_to_image(tensor):
  tensor = tensor*255
  tensor = np.array(tensor, dtype=np.uint8)
  if np.ndim(tensor)>3:
    assert tensor.shape[0] == 1
    tensor = tensor[0]
  return PIL.Image.fromarray(tensor)

def loadDataset(dataset_dir, batch_size):
  dataset = tf.keras.utils.image_dataset_from_directory(
    dataset_dir,
    labels=None,
    label_mode=None,
    batch_size=batch_size,
    image_size=(dim,dim),
    crop_to_aspect_ratio=True
  )

  return dataset

def load_img(path_to_img):
  img = tf.io.read_file(path_to_img)
  img = tf.image.decode_image(img, channels=3)
  img = tf.image.convert_image_dtype(img, tf.float32)
  img = tf.image.central_crop(img, 1)
  img = tf.image.resize(img, [dim,dim])
  img = img[tf.newaxis, :]
  return img

initializer = tf.random_normal_initializer(
    mean=0.0, stddev=0.01, seed=None
)
betaInitializer = initializers.constant(0.)
gammaInitializer = initializers.constant(1.)

class ConditionalInstanceNorm(tf.keras.layers.Layer):
  def __init__(self, scope_bn, yShape):
    super(ConditionalInstanceNorm, self).__init__()
    self.scope_bn = scope_bn
    self.yShape = yShape
  
  def build(self, input_shape):
    self.beta = self.add_weight(name="beta"+self.scope_bn, shape=(self.yShape[-1], input_shape[-1]), initializer=betaInitializer, trainable=True)
    self.gamma = self.add_weight(name="gamma"+self.scope_bn, shape=(self.yShape[-1], input_shape[-1]), initializer=gammaInitializer, trainable=True)
  
  def call(self, inputs, y1, y2, alpha):
    mean, var = tf.nn.moments(x=inputs, axes=[1,2], keepdims=True)
    beta1 = tf.matmul(y1, self.beta)
    gamma1 = tf.matmul(y1, self.gamma)
    beta2 = tf.matmul(y2, self.beta)
    gamma2 = tf.matmul(y2, self.gamma)
    beta = alpha*beta1 + (1. - alpha)*beta2
    gamma = alpha*gamma1 + (1. - alpha)*gamma2
    x = tf.nn.batch_normalization(x=inputs, mean=mean, variance=var, offset=beta, scale=gamma, variance_epsilon=1e-5)
    return x

def PadConvBatch(x, filters=32, kernel_size=3, strides=1, activation='relu', scope_bn="", y1=None, y2=None, alpha=1):
  if isinstance(strides, float):
    x = UpSampling2D(size=(2,2), data_format="channels_last", interpolation='nearest')(x)
    strides=1
  padding = kernel_size//2
  x = tf.pad(x, [[0,0], [padding,padding], [padding,padding], [0,0]], "REFLECT")
  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, activation=activation, kernel_initializer=initializer)(x)
  conditionalInstanceNorm = ConditionalInstanceNorm(scope_bn=scope_bn, yShape=y1.shape)
  x = conditionalInstanceNorm(x, y1, y2, alpha)
  return x

def Resblock(x, scope_bn1="", scope_bn2="", y1=None, y2=None, alpha=1):
  fx = PadConvBatch(x=x, filters=128, activation="linear", scope_bn=scope_bn1, y1=y1, y2=y2, alpha=alpha)
  fx = ReLU()(fx)
  fx = PadConvBatch(x=fx, filters=128, activation="linear", scope_bn=scope_bn2, y1=y1, y2=y2, alpha=alpha)
  out = Add()([x,fx])
  return out

def ImageTransformNetwork():
  inputs = Input(shape=(dim,dim,3))
  y1 = Input(shape=(10), batch_size=1)
  y2 = Input(shape=(10), batch_size=1)
  alpha = Input(shape=(1), batch_size=1)
  x = PadConvBatch(x=inputs, kernel_size=9, scope_bn="1", y1=y1, y2=y2, alpha=alpha) 
  x = PadConvBatch(x=x, filters=64, strides=2, scope_bn="2", y1=y1, y2=y2, alpha=alpha)
  x = PadConvBatch(x=x, filters=128, strides=2, scope_bn="3", y1=y1, y2=y2, alpha=alpha)
  x = Resblock(x=x, scope_bn1="4", scope_bn2="5", y1=y1, y2=y2, alpha=alpha)
  x = Resblock(x=x, scope_bn1="6", scope_bn2="7", y1=y1, y2=y2, alpha=alpha)
  x = Resblock(x=x, scope_bn1="8", scope_bn2="9", y1=y1, y2=y2, alpha=alpha)
  x = Resblock(x=x, scope_bn1="10", scope_bn2="11", y1=y1, y2=y2, alpha=alpha)
  x = Resblock(x=x, scope_bn1="12", scope_bn2="13", y1=y1, y2=y2, alpha=alpha)
  x = PadConvBatch(x=x, filters=64, strides=0.5, scope_bn="14", y1=y1, y2=y2, alpha=alpha)
  x = PadConvBatch(x=x, filters=32, strides=0.5, scope_bn="15", y1=y1, y2=y2, alpha=alpha)
  x = tf.pad(x, [[0,0], [4,4], [4,4], [0,0]], "REFLECT")
  x = Conv2D(filters=3, kernel_size=9, strides=1, activation='sigmoid', kernel_initializer=initializer)(x)
  model = Model(inputs=[inputs, y1, y2, alpha], outputs=x)
  return model

os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'
content_layers = ['block4_conv3']
style_layers = ['block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3']
num_content_layers = len(content_layers)
num_style_layers = len(style_layers)

def vgg_layers(layer_names):
  vgg = tf.keras.applications.VGG16(include_top=False, weights='imagenet')

  outputs = [vgg.get_layer(name).output for name in layer_names]

  model = tf.keras.Model([vgg.input], outputs)
  return model

def gram_matrix(input_tensor):
  shape = tf.shape(input_tensor)
  num_images = shape[0]
  width = shape[1]
  height = shape[2]
  num_filters = shape[3]
  filters = tf.reshape(input_tensor, tf.stack([num_images, -1, num_filters]))
  grams = tf.matmul(filters, filters, transpose_a=True) / tf.cast(width * height * num_filters, dtype=tf.float32)
  return grams

class LossNetwork(tf.keras.models.Model):
  def __init__(self, style_layers, content_layers):
    super(LossNetwork, self).__init__()
    self.vgg = vgg_layers(style_layers+content_layers)
    self.style_layers = style_layers
    self.content_layers = content_layers
    self.num_style_layers = len(style_layers)
    self.vgg.trainable = False
  
  def call(self, inputs):
    inputs = inputs*255.0
    preprocessed_input = tf.keras.applications.vgg16.preprocess_input(inputs)
    outputs = self.vgg(preprocessed_input)
    style_outputs, content_outputs = (outputs[:self.num_style_layers], outputs[self.num_style_layers:])

    style_outputs = [gram_matrix(style_output) for style_output in style_outputs]
    content_dict = {content_name: value for content_name, value in zip(self.content_layers, content_outputs)}
    style_dict = {style_name: value for style_name, value in zip(self.style_layers, style_outputs)}
    return {'content': content_dict, 'style': style_dict}

def style_content_loss(outputs, style_targets, content_targets, style_weight, content_weight):
  style_outputs = outputs["style"]
  content_outputs = outputs["content"]

  style_loss_total = 0
  for name in style_outputs.keys():
    size = tf.cast(tf.size(style_outputs[name]), tf.float32)
    style_loss = tf.nn.l2_loss(style_outputs[name] - style_targets[name])*2/size
    style_loss_total += style_loss
  
  content_loss_total = 0
  for name in content_outputs.keys():
    size = tf.cast(tf.size(content_outputs[name]), tf.float32)
    content_loss = tf.nn.l2_loss(content_outputs[name] - content_targets[name])*2/size
    content_loss_total += content_loss

  loss = style_loss_total*style_weight + content_loss_total*content_weight
  return loss

def loadLossNetwork():
  return LossNetwork(style_layers, content_layers)

style_image_dict = {
    "1.png": 0,
    "2.png": 1,
    "3.png": 2,
    "4.png": 3,
    "5.png": 4,
    "6.png": 5,
    "7.png": 6,
    "8.png": 7,
    "9.png": 8,
    "10.png": 9
}

length = len(style_image_dict)

def loadSavedModel(modelName):
  export_dir = "/content/drive/MyDrive/EPQ/Saved_Models/"
  path = export_dir + modelName
  imported = tf.keras.models.load_model(path)
  return imported

def inferAndSave(num, ylabel1="1.png", ylabel2="2.png", model="", alpha=1.):
  y1 = np.zeros([1,length])
  y1[0, style_image_dict[ylabel1]] = 1
  y2 = np.zeros([1,length])
  if alpha != 1.:
    y2[0, style_image_dict[ylabel2]] = 1
  alpha = tf.constant([alpha])

  image = load_img("/content/drive/MyDrive/EPQ/Transformed_Images/test.png")
  stylised_tensor = model([image, y1, y2, alpha])
  stylised_image = tensor_to_image(stylised_tensor)
  stylised_image.save("/content/drive/MyDrive/EPQ/Transformed_Images/savedImage"+str(num)+".png")

def saveModel(model, step, final):
  export_dir = "/content/drive/MyDrive/EPQ/Saved_Models/"
  export_dir = export_dir + "model" + str(step)
  if final:
    converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
    tflite_model = converter.convert()
    tflite_model_file = pathlib.Path("/content/drive/MyDrive/EPQ/Saved_Models/modelFinal"+str(step)+".tflite")
    tflite_model_file.write_bytes(tflite_model)
  else:
    os.mkdir(export_dir)
    model.save(export_dir)

def train(styleImage="1.png", style_weight=0.1, content_weight=1.0, batch_size=16, imported="", freeze=True):
  train_dataset = loadDataset("/content/train2014", batch_size)

  y1 = np.zeros([1,length])
  y1[0,style_image_dict[styleImage]] = 1
  y2 = np.zeros([1,length])
  alpha = tf.constant([1.])

  if imported != "":
    imageTransformNetwork = loadSavedModel(imported)
  else:
    imageTransformNetwork = ImageTransformNetwork()
  
  if freeze:
    for layer in imageTransformNetwork.layers:
      if "conditional_instance_norm" not in layer.name:
        layer.trainable = False

  lossNetwork = loadLossNetwork()

  style_image = load_img("/content/drive/MyDrive/EPQ/Style_Images/"+str(style_image_dict[styleImage]+1)+".png")
  style_image = tf.repeat(style_image, batch_size, axis=0)
  style_targets = lossNetwork(style_image)["style"]

  opt = tf.keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-1)
  step = 15001
  for epochs in range(1):
    for i, batch in enumerate(train_dataset):
      step += 1

      batch = rescaling(batch)

      with tf.GradientTape() as tape:
        stylised = imageTransformNetwork([batch, y1, y2, alpha], training=True)
        outputs = lossNetwork(stylised)
        content_targets = lossNetwork(batch)["content"]

        loss = style_content_loss(outputs, style_targets, content_targets, style_weight, content_weight)
      
      gradients = tape.gradient(loss, imageTransformNetwork.trainable_weights)
      grad_clipped = [(tf.clip_by_norm(grad, 1.0)) for grad in gradients]
      opt.apply_gradients(zip(grad_clipped, imageTransformNetwork.trainable_weights))

      if step%50 == 0:
        tf.print(loss)

      if step%500 == 0:
        saveModel(imageTransformNetwork, step, False)
        inferAndSave(step, ylabel1=styleImage, model=imageTransformNetwork)
        break
  
  # saveModel(imageTransformNetwork, step, True)

rescaling = Rescaling(1./255)
train(styleImage="6.png", imported="model15000", freeze=True)

imageTransformNetwork = loadSavedModel("model11500")

print(imageTransformNetwork.layers[6].weights)

inferAndSave(1, ylabel1="2.png", ylabel2="1.png", model=imageTransformNetwork, alpha=1)

converter = tf.lite.TFLiteConverter.from_saved_model("/content/drive/MyDrive/EPQ/Saved_Models/model15500")
tflite_model = converter.convert()
tflite_model_file = pathlib.Path("/content/drive/MyDrive/EPQ/Saved_Models/modelFinal15500.tflite")
tflite_model_file.write_bytes(tflite_model)